---
layout: ../layouts/Layout.astro
title: "LiveOCRBench: Tracking Vision-Language Models' Persistent Struggles with Text Recognition"
description: "LiveOCRBench is a benchmark for evaluating the OCR abilities of vision-language models on different fonts, designs, and languages."
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import benchmark from "../assets/benchmark.png"
import bias from "../assets/bias.png"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Yesheng Liu",
      notes: ["*", "†", "††"],
    },
    {
      name: "Zheqi He",
      institution: "",
      notes: ["*"],
    },
    {
      name: "Jingshu Zheng",
      notes: ["*"],
    },
    {
      name: "Jinge Yao",
      notes: ["*"],
    },
    {
      name: "Xi Yang",
      notes: ["*"],
    },
    {
      name: "Jiajun Zhang",
      notes: ["†", "††"],
    },
  ]}
  notes={[
    {
      symbol: "*",
      text: "BAAI",
    },
    {
      symbol: "†",
      text: "Institute of Automation, Chinese Academy of Sciences",
    },
    {
      symbol: "††",
      text: "School of Artificial Intelligence, University of Chinese Academy of Sciences ",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/RomanHauksson/academic-project-astro-template",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
    {
      name: "Dataset",
      url: "xx",
      icon: "simple-icons:huggingface",
      iconColor: "#FFD21E",
    }
  ]}
  />


<HighlightedSection>

## Abstract

In recent years, Large Multimodal Models (LMMs) have been widely used across a variety of tasks. In particular, their capabilities in recognizing textual content have been strong, judging from high metrics of text-centric benchmarks. However, there are relatively fewer attempts to specifically benchmark text recognition over various types of fonts or design. Moreover, as LMMs typically incorporate extensive web data in training, the high scores observed in existing benchmarks may heavily stem from data contamination. To effectively mitigate such contamination and to accurately evaluate the true OCR capabilities of LMMs, we present LiveOCRBench, a dynamic benchmark designed specifically for this purpose. LiveOCRBench leverages continuously updated visual content and the corresponding meta-data across four diverse categories, utilizing a semi-automated pipeline to select images that effectively challenge the OCR capabilities of models, significantly reducing the costs associated with direct manual annotation.

We evaluated 25 state-of-the-art LMMs and found that even more recent, advanced models may struggle with simple title queries. Our analysis reveals that current LMMs heavily rely on recalling textual content from previously seen images rather than accurately recognizing individual characters. We will publicly release both our benchmark data and evaluation suite, and plan for future updates on a quarterly basis, to continuously track the performance of various LMMs.
</HighlightedSection>

## Figures

Use the figure component to display images, videos, equations, or any other element, with an optional caption.

<Figure>
  <Image slot="figure" source={benchmark} altText="LiveOCRBench" />
  <span slot="caption">Examples from LiveOCRBench, encompassing four distinct categories: Book, Movie, Album, and Game. Each category includes representative visual examples along with their title(author if available)</span>
</Figure>

## LMMs struggle with simple title queries
Examples illustrating the Language Bias of LMMs, where the models produced typos in these specific, human-simple cases
<Figure>
  <Image slot="figure" source={bias} altText="Language Bias" />
  <span slot="caption">Examples illustrating the Language Bias of LMMs, where the models produced typos in these specific, human-simple cases</span>
</Figure>

## Leaderboard

| Model | Overall | Album | Book | Game | Movie |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Gemini-2.0-flash | **76.86\%** | **66.67\%** | 84.71\% | *77.78\%* | 75.76\% |
| GPT-4o | *76.42\%* | 51.52\% | **90.59\%** | **80.00\%** | 84.85\% |
| Qwen2-VL-7B | 75.11\% | *63.64\%* | 85.88\% | 68.89\% | 78.79\% |
| Gemini-1.5-pro | 73.36\% | *63.64\%* | 84.71\% | 66.67\% | 72.73\% |
| GPT-4o-mini | 72.93\% | 53.03\% | *88.24\%* | 66.67\% | 81.82\% |
| Qwen2.5-VL-72B | 65.94\% | 43.94\% | 72.94\% | 66.67\% | **90.91\%** |
| Qwen2.5-VL-7B | 65.94\% | 45.45\% | 77.65\% | 64.44\% | 78.79\% |
| Qwen2-VL-72B | 65.07\% | 50.00\% | 68.24\% | 64.44\% | *87.88\%* |
| Claude-3-7-sonnet | 64.63\% | 30.30\% | 82.35\% | 64.44\% | *87.88\%* |
| Qwen2-VL-2B | 63.76\% | 51.52\% | 72.94\% | 57.78\% | 72.73\% |
| Claude-3-5-sonnet | 60.70\% | 28.79\% | 71.76\% | 68.89\% | 84.85\% |
| Pixtral-Large | 60.26\% | 31.82\% | 77.65\% | 55.56\% | 78.79\% |
| InternVL2\_5-78B | 50.22\% | 40.91\% | 51.76\% | 51.11\% | 63.64\% |
| InternVL2-Llama3-76B | 50.22\% | 40.91\% | 56.47\% | 40.00\% | 66.67\% |
| MiniCPM-V-2\_6 | 49.34\% | 28.79\% | 58.82\% | 53.33\% | 60.61\% |
| LLaVA-OV-7b | 46.72\% | 43.94\% | 49.41\% | 37.78\% | 57.58\% |
| Molmo-7B-D | 45.85\% | 34.85\% | 54.12\% | 46.67\% | 45.45\% |
| InternVL2\_5-8B | 44.98\% | 30.30\% | 50.59\% | 46.67\% | 57.58\% |
| Molmo-72B | 42.36\% | 37.88\% | 45.88\% | 44.44\% | 39.39\% |
| Phi-3.5-vision | 37.55\% | 19.70\% | 38.82\% | 51.11\% | 51.52\% |
| InternVL2-8B | 36.68\% | 22.73\% | 45.88\% | 35.56\% | 42.42\% |
| NVLM-D-72B | 34.06\% | 31.82\% | 30.59\% | 40.00\% | 39.39\% |
| Idefics3-8B-Llama3 | 27.95\% | 28.79\% | 30.59\% | 24.44\% | 24.24\% |
| Meta-Llama-3.2-11B | 13.54\% | 7.58\% | 11.76\% | 26.67\% | 12.12\% |
| Meta-Llama-3.2-90B | 10.92\% | 7.58\% | 21.18\% | 0.00\% | 6.06\% |

## BibTeX citation

```bibtex
@misc{liu2025liveocrbench,
  author = "{Yesheng Liu, Zheqi He, Jingshu Zheng, Jinge Yao, Xi Yang, Jiajun Zhang}",
  title = "LiveOCRBench: Tracking Vision-Language Models' Persistent Struggles with Text Recognition",
  year = "2025",
  howpublished = "\url{https://flageval-baai.github.io/liveocrbench/}",
}
```
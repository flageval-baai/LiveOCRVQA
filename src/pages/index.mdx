---
layout: ../layouts/Layout.astro
title: "LiveOCRVQA: Mitigating Data Contamination to Test True LMM Stylized Text Reading"
description: "LiveOCRVQA is a benchmark for evaluating the abilities of vision-language models on stylized text reading."
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import benchmark from "../assets/benchmark.png"
import bias from "../assets/bias.png"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Yesheng Liu",
      notes: ["*", "†", "††"],
    },
    {
      name: "Zheqi He",
      institution: "",
      notes: ["*"],
    },
    {
      name: "Jingshu Zheng",
      notes: ["*"],
    },
    {
      name: "Jinge Yao",
      notes: ["*"],
    },
    {
      name: "Xi Yang",
      notes: ["*"],
    },
    {
      name: "Jiajun Zhang",
      notes: ["†", "††"],
    },
  ]}
  notes={[
    {
      symbol: "*",
      text: "BAAI",
    },
    {
      symbol: "†",
      text: "Institute of Automation, Chinese Academy of Sciences",
    },
    {
      symbol: "††",
      text: "School of Artificial Intelligence, University of Chinese Academy of Sciences ",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/RomanHauksson/academic-project-astro-template",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
    {
      name: "Dataset",
      url: "https://huggingface.co/datasets/BAAI/LiveOCRVQA/",
      icon: "simple-icons:huggingface",
      iconColor: "#FFD21E",
    }
  ]}
  />


<HighlightedSection>

## Abstract

Large Multimodal Models (LMMs) exhibit impressive text recognition performance on current visual question answering (VQA) and document-centric VQA benchmarks, which predominantly feature text in standardized, print-like formats. This methodological focus fails to adequately capture the diverse, often stylized fonts encountered in real-world scenarios, such as those in artistic designs and web media. While some datasets incorporate images with such complex text, their reliance on older, static image collections, risking data contamination from LMMs' extensive web pretraining. To effectively mitigate such contamination and rigorously evaluate true text-in-image capabilities of LMMs, we introduce LiveOCRVQA. This dynamic benchmark leverages continuously updated visual content and the corresponding meta-data across four diverse categories. It utilizes a semi-automated pipeline to curate images containing stylized text that, while often easily deciphered by humans, effectively challenges the models' underlying text processing abilities, yielding an initial benchmark of 385 such instances. . Our evaluation of 21 prominent models reveals that even recent, advanced models struggle significantly  with queries involving stylized text from novel content. This performance disparity  suggests that high scores on previous, more standardized benchmarks may not accurately reflect robust text recognition capabilities for varied styles. Indeed our analysis indicates that current LMMs often rely on recalling textual content from previously seen images rather than performing fine-grained character recognition on unfamiliar or complex font styles. We will publicly release both our benchmark data and evaluation suite, and plan for future updates on a quarterly basis, to continuously track the performance of various LMMs.
</HighlightedSection>

## Figures

Use the figure component to display images, videos, equations, or any other element, with an optional caption.

<Figure>
  <Image slot="figure" source={benchmark} altText="LiveOCRBench" />
  <span slot="caption">Examples from LiveOCRBench, encompassing four distinct categories: Book, Movie, Album, and Game. Each category includes representative visual examples along with their title(author if available)</span>
</Figure>

## LMMs struggle with simple title queries
Examples illustrating the Language Bias of LMMs, where the models produced typos in these specific, human-simple cases
<Figure>
  <Image slot="figure" source={bias} altText="Language Bias" />
  <span slot="caption">Examples illustrating the Language Bias of LMMs, where the models produced typos in these specific, human-simple cases</span>
</Figure>

## Leaderboard

| Model | Overall | Album | Book | Game | Movie |
| :---: | :---: | :---: | :---: | :---: | :---: |
| Gemini-2.0-flash | **76.86\%** | **66.67\%** | 84.71\% | *77.78\%* | 75.76\% |
| GPT-4o | *76.42\%* | 51.52\% | **90.59\%** | **80.00\%** | 84.85\% |
| Qwen2-VL-7B | 75.11\% | *63.64\%* | 85.88\% | 68.89\% | 78.79\% |
| Gemini-1.5-pro | 73.36\% | *63.64\%* | 84.71\% | 66.67\% | 72.73\% |
| GPT-4o-mini | 72.93\% | 53.03\% | *88.24\%* | 66.67\% | 81.82\% |
| Qwen2.5-VL-72B | 65.94\% | 43.94\% | 72.94\% | 66.67\% | **90.91\%** |
| Qwen2.5-VL-7B | 65.94\% | 45.45\% | 77.65\% | 64.44\% | 78.79\% |
| Qwen2-VL-72B | 65.07\% | 50.00\% | 68.24\% | 64.44\% | *87.88\%* |
| Claude-3-7-sonnet | 64.63\% | 30.30\% | 82.35\% | 64.44\% | *87.88\%* |
| Qwen2-VL-2B | 63.76\% | 51.52\% | 72.94\% | 57.78\% | 72.73\% |
| Claude-3-5-sonnet | 60.70\% | 28.79\% | 71.76\% | 68.89\% | 84.85\% |
| Pixtral-Large | 60.26\% | 31.82\% | 77.65\% | 55.56\% | 78.79\% |
| InternVL2\_5-78B | 50.22\% | 40.91\% | 51.76\% | 51.11\% | 63.64\% |
| InternVL2-Llama3-76B | 50.22\% | 40.91\% | 56.47\% | 40.00\% | 66.67\% |
| MiniCPM-V-2\_6 | 49.34\% | 28.79\% | 58.82\% | 53.33\% | 60.61\% |
| LLaVA-OV-7b | 46.72\% | 43.94\% | 49.41\% | 37.78\% | 57.58\% |
| Molmo-7B-D | 45.85\% | 34.85\% | 54.12\% | 46.67\% | 45.45\% |
| InternVL2\_5-8B | 44.98\% | 30.30\% | 50.59\% | 46.67\% | 57.58\% |
| Molmo-72B | 42.36\% | 37.88\% | 45.88\% | 44.44\% | 39.39\% |
| Phi-3.5-vision | 37.55\% | 19.70\% | 38.82\% | 51.11\% | 51.52\% |
| InternVL2-8B | 36.68\% | 22.73\% | 45.88\% | 35.56\% | 42.42\% |
| NVLM-D-72B | 34.06\% | 31.82\% | 30.59\% | 40.00\% | 39.39\% |
| Idefics3-8B-Llama3 | 27.95\% | 28.79\% | 30.59\% | 24.44\% | 24.24\% |
| Meta-Llama-3.2-11B | 13.54\% | 7.58\% | 11.76\% | 26.67\% | 12.12\% |
| Meta-Llama-3.2-90B | 10.92\% | 7.58\% | 21.18\% | 0.00\% | 6.06\% |

## BibTeX citation

```bibtex
@misc{liu2025liveocrbench,
  author = "{Yesheng Liu, Zheqi He, Jingshu Zheng, Jinge Yao, Xi Yang, Jiajun Zhang}",
  title = "LiveOCRBench: Tracking Vision-Language Models' Persistent Struggles with Text Recognition",
  year = "2025",
  howpublished = "\url{https://flageval-baai.github.io/liveocrbench/}",
}
```